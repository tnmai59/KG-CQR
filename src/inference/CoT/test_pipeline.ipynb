{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a5966f-cda1-4e3f-9f89-ecbe9e4127b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CALL API ENPOINTS (LLM, EMBEDDING)\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1872d2-3aac-45ec-a15a-4ae55473e49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "os.environ[\"http_proxy\"] = \"\"\n",
    "os.environ[\"https_proxy\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3423138-d290-42bb-b838-17abcbfde695",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CALL LLM\n",
    "from transformers import AutoTokenizer\n",
    "from langchain_community.llms import VLLMOpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from config import *\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "### For Chat OpenAI template\n",
    "llm = ChatOpenAI(\n",
    "\tmodel=MODEL_NAME,\n",
    "\topenai_api_key=\"EMPTY\",\n",
    "\tmax_tokens=512,\n",
    "\topenai_api_base=INFERENCE_SERVER_URL,\n",
    "\ttemperature=0,\n",
    "\tstreaming= False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f36c8c-e15f-425f-9ab0-b004d20e24af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from beam import *\n",
    "from transformers import AutoTokenizer\n",
    "ebd_tok = AutoTokenizer.from_pretrained(\"embedding_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ae425c-e4f9-4b7d-a0d0-6614fc00fb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Embeeding\n",
    "\n",
    "### Call API Endpoint Embedding\n",
    "import json\n",
    "import requests\n",
    "from typing import List\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def max_len(query):\n",
    "\tt = ebd_tok.encode(query)\n",
    "\tif len(t) > 512:\n",
    "\t\tt = t[:500]\n",
    "\t\tquery = ebd_tok.decode(t)\n",
    "\treturn query\n",
    "\t\n",
    "class CustomAPIEmbeddings(Embeddings):\n",
    "\tdef __init__(self, api_url: str, show_progress: bool):\n",
    "\t\tself.api_url = api_url\n",
    "\t\tself.show_progress = show_progress\n",
    "\n",
    "\tdef embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "\t\tlst_embedding = []\n",
    "\t\tif self.show_progress:  # for tqdm embedding\n",
    "\t\t\tfor query in tqdm(texts):\n",
    "\t\t\t\t# query = max_len(query)\n",
    "\t\t\t\tpayload = json.dumps({\n",
    "\t\t\t\t\t\"inputs\": query\n",
    "\t\t\t\t})\n",
    "\t\t\t\theaders = {\n",
    "\t\t\t\t\t'Content-Type': 'application/json'\n",
    "\t\t\t\t}\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tresponse = json.loads(\n",
    "\t\t\t\t\t\trequests.request(\"POST\", self.api_url, headers=headers, data=payload).text\n",
    "\t\t\t\t\t)\n",
    "\t\t\t\t\tlst_embedding.append(response[0])\n",
    "\t\t\t\texcept Exception as e:\n",
    "\t\t\t\t\tprint(f\"Error: {e}\")\n",
    "\t\t\t\t\tprint(requests.request(\"POST\", self.api_url, headers=headers, data=payload).text)\n",
    "\t\telse:\n",
    "\t\t\tfor query in texts:\n",
    "\t\t\t\tquery = max_len(query)\n",
    "\t\t\t\tpayload = json.dumps({\n",
    "\t\t\t\t\t\"inputs\": query\n",
    "\t\t\t\t})\n",
    "\t\t\t\theaders = {\n",
    "\t\t\t\t\t'Content-Type': 'application/json'\n",
    "\t\t\t\t}\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tresponse = json.loads(\n",
    "\t\t\t\t\t\trequests.request(\"POST\", self.api_url, headers=headers, data=payload).text\n",
    "\t\t\t\t\t)\n",
    "\t\t\t\t\tlst_embedding.append(response[0])\n",
    "\t\t\t\texcept Exception as e:\n",
    "\t\t\t\t\tprint(f\"Error: {e}\")\n",
    "\t\t\t\t\t# print(requests.request(\"POST\", self.api_url, headers=headers, data=payload).text)\n",
    "\n",
    "\t\treturn lst_embedding\n",
    "\n",
    "\tdef embed_query(self, text: str) -> List[float]:\n",
    "\t\treturn self.embed_documents([text])[0]\n",
    "\n",
    "# Instantiate\n",
    "embeddings = CustomAPIEmbeddings(api_url=INFERENCE_SERVER_URL, show_progress=False)\n",
    "\n",
    "\n",
    "### test here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82420213-5a13-44ae-90bd-6844c572bea1",
   "metadata": {},
   "source": [
    "### 1. Load Graph Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16aa1b1-4bb8-4336-949a-26a6a015c274",
   "metadata": {},
   "source": [
    "#### Load Data (Triplets, Triplets Relation Embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63915c9b-798e-4ab5-a6ed-c5256d676836",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "with open(TRIPLET_MAP_PATH,'rb') as f:\n",
    "\tdct_mapping_triplet = pickle.load(f)\n",
    "\n",
    "with open(TRIPLET_EMB_PATH,'rb') as f:\n",
    "\tlst_embedding = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd2c7b5-602d-4779-924c-53327fd36a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### convert pickle file to numpy\n",
    "\n",
    "import numpy as np\n",
    "lst_embedding = np.array(lst_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3382c9a-ac7a-4eb1-80cd-ce6e3931f36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_test = pd.read_csv(\"final_data.csv\")\n",
    "test_data = df_test['question'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4694d341-f160-4528-baf6-5f19871e47eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "faiss_embeddings = lst_embedding.astype('float32')\n",
    "d = faiss_embeddings.shape[1] \n",
    "index = faiss.IndexFlatIP(d)\n",
    "index.add(faiss_embeddings) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d21a29-19d1-4db9-9043-7311c364f0b3",
   "metadata": {},
   "source": [
    "### 2. Contextxual Question Retrieval (CQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f001590",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import defaultdict, namedtuple\n",
    "\n",
    "model = embeddings\n",
    "Triplet = namedtuple(\"Triplet\", [\"head\", \"relation\", \"tail\", \"ttr\"])\n",
    "KG_list = [\n",
    "\t\tTriplet(rec[\"r\"][0][\"id\"], rec[\"r\"][1], rec[\"r\"][2][\"id\"], rec[\"r.summary\"])\n",
    "\t\tfor rec in dct_mapping_triplet\n",
    "\t]\n",
    "KG = build_undirected_graph(KG_list)\n",
    "all_summaries = {edge[\"r\"][\"summary\"] for edges in KG.values() for edge in edges}\n",
    "summary_embeddings = dict(\n",
    "\t\tzip(\n",
    "\t\t\tlist(all_summaries),\n",
    "\t\t\tmodel.embed_query(list(all_summaries))\n",
    "\t\t)\n",
    "\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a33bc8-4b12-4191-96b8-d311f5c1cdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain.prompts import PromptTemplate\n",
    "from typing import Literal\n",
    "import multiprocessing\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "def faiss_cosine(index, query_vector, k=10):\n",
    "\tquery_vector = query_vector.astype('float32')\n",
    "\tdistances, indices = index.search(query_vector, k)\n",
    "\treturn indices.flatten()\n",
    "\n",
    "\t\n",
    "def max_len(query):\n",
    "\tt = ebd_tok.encode(query)\n",
    "\tif len(t) > 512:\n",
    "\t\tt = t[:500]\n",
    "\t\tquery = ebd_tok.decode(t)\n",
    "\treturn query\n",
    "\n",
    "def query_triplet_topk(query, k=10):\n",
    "\tquery = max_len(query)\n",
    "\tquery_emb = np.array(embeddings.embed_query(query)).reshape(1,-1)\n",
    "\t# similarities = cosine_similarity(query_emb, lst_embedding).flatten()\n",
    "\ttopk_indices_sorted = faiss_cosine(query_emb).tolist()\n",
    "\treturn [dct_mapping_triplet[x] for x in topk_indices_sorted]\n",
    "\n",
    "\n",
    "class GradeRelationList(BaseModel):\n",
    "\t\"\"\"List passage index check on retrieved text.\"\"\"\n",
    "\tpassage_idx: str = Field(\n",
    "\t\tdescription=\"The passage index of relevant chunks, seperated by a comma\"\n",
    "\t)\n",
    "\n",
    "def check_grade_lst(question, text):\n",
    "\tprompt_text_grader = PromptTemplate(template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing relevance \n",
    "\t\tof a list of retrieved passages to a user question. The goal is to filter out erroneous retrievals. \\n\n",
    "\t\tReturn only the passage index whether the passage is relevant to the question. \\n\n",
    "\t\tProvide the output as a JSON with passage index seperated by a comma and no premable or explaination.\n",
    "\t\t <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\t\tHere is the list of retrieved text: \\n\\n {text} \\n\\n\n",
    "\t\tHere is the user question: {question} \\n <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\t\t\"\"\",\n",
    "\t\tinput_variables=[\"question\", \"text\"]\n",
    "\t)\n",
    "\t# retrieval_grader = LLMChain(prompt=prompt, llm=llm)\n",
    "\tstructured_llm_grader = llm.with_structured_output(GradeRelationList)\n",
    "\trelation_grader = prompt_text_grader | structured_llm_grader \n",
    "\tresult = relation_grader.invoke({\"question\": question, \"text\": text})\n",
    "\t# print(result)\n",
    "\treturn result\n",
    "\n",
    "\n",
    "def format_relations(relations):\n",
    "\tresult = []\n",
    "\tfor rel in relations:\n",
    "\t\tformatted_relation = f\"{rel['n']['id']} - {rel['r'][1]} -> {rel['m']['id']}\"\n",
    "\t\tresult.append(formatted_relation)\n",
    "\treturn result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411cb9c3-501f-4e7e-8775-d2910183ad6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback, time\n",
    "\n",
    "cnt_err = 0\n",
    "def format_claim(relations):\n",
    "\treturn \"\\n\\n\".join(f\"{idx+1}. {rel['r']['summary']}\" for idx, rel in enumerate(relations))\n",
    "\n",
    "def format_triplet(relations):\n",
    "\treturn \"\\n\\n\".join(f\"{idx+1}. ({rel['r'][0]['id']}, {rel['r'][1]}, {rel['r'][2]['id']})\" for idx, rel in enumerate(relations))\n",
    "\n",
    "\n",
    "class contextual_output(BaseModel):\n",
    "\t\"\"\"contextual summarization for the input question.\"\"\"\n",
    "\tsummary: str = Field(\n",
    "\t\tdescription=\"Concise summary ocontextual information of the input question\"\n",
    "\t)\n",
    "\n",
    "class contextual_triplets(BaseModel):\n",
    "\t\"\"\"contextual generation of knowledge subgraph.\"\"\"\n",
    "\tcontext: str = Field(\n",
    "\t\tdescription=\"generate concise contextual information based on list of triplets. Output MUST be VALID JSON\"\n",
    "\t)\n",
    "\t\n",
    "\n",
    "def contextual_question_retrieval(claims):\n",
    "\tsystem_promt=(\"You are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\"\n",
    "\t\" Given the list of claims that may relation with each other. Please write a Concise summary of claims that aim to provide a contextual information.\"\n",
    "\t\" The output just generate a concise summary without any explaination.\"\n",
    "\t\" Please note that if the provided claims are contradictory, please resolve the contradictions and provide a single, coherent summary (no need Here is part)\")\n",
    "\tchat_template_contextual = tokenizer.apply_chat_template(\n",
    "\t\t[\n",
    "\t\t\t{\"role\":\"system\", \"content\":\"{system}\"},\n",
    "\t\t\t{\"role\":\"user\", \"content\":\"\\nHere is the list of claims {claims}\\n\"}\n",
    "\t\t], tokenize=False, add_generation_prompt=True)\n",
    "\t\n",
    "\tprompt_summary_contextual = PromptTemplate(template=chat_template_contextual, input_variables=[\"system\", \"claims\"])\n",
    "\tstructured_summary_contextual = llm.with_structured_output(contextual_output)\n",
    "\tcontextual_chain = prompt_summary_contextual | structured_summary_contextual \n",
    "\tresults = contextual_chain.invoke({\"system\": system_promt, \"claims\": claims})\n",
    "\treturn results\n",
    "\n",
    "\n",
    "def add_triplet_context_to_question(KG, model, summary_embeddings, question):\n",
    "\t# global KG\n",
    "\t# global model\n",
    "\t# global summary_embeddings\n",
    "\trelations = query_triplet_topk(question)\n",
    "\tT = format_T(relations)### check all relations in one LLM call\n",
    "\ttry:\n",
    "\t\tcontext = check_grade_lst(question, format_claim(T)).passage_idx\n",
    "\t\tcontext = [int(x) for x in context.split(\",\")]\n",
    "\t\tT = [T[x-1] for x in context]\n",
    "\t\tif T == []:\n",
    "\t\t\tcontextual_summary = \"\"\n",
    "\t\telse:\n",
    "\t\t\tH = relevance_guided_path_addition_beam(KG, T, model, summary_embeddings, 20, 20, 2)\n",
    "\t\t\tcontextual_summary = contextual_question_retrieval(format_claim(H)).summary\n",
    "\texcept Exception as e:\n",
    "\t\tcontextual_summary = \"\"\n",
    "\tif contextual_summary != \"\":\n",
    "\t\tquestion = question + \" with some extra data: \" + contextual_summary\n",
    "\treturn question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9813e40f-26ba-49ca-a9b3-96a00b7ac1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_triplet_top_k_cos = []\n",
    "for i in tqdm(test_data):\n",
    "\tlst_triplet_top_k_cos.append(query_triplet_topk(i))\n",
    "\n",
    "map_triplet = {}\n",
    "for i,j in zip(lst_triplet_top_k_cos, test_data):\n",
    "\tmap_triplet[j] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdb4e5b-a82b-4f35-b6c1-b468a6783b5b",
   "metadata": {},
   "source": [
    "### 3. CQR for Multi-Step Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e6ab8b-8030-49ab-928a-743a6cc4e7a2",
   "metadata": {},
   "source": [
    "#### 3.1 Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a80c2f0-9b75-4000-88fd-c311cf084b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()\n",
    "docs_corpus = df_test[\"documents\"].tolist()\n",
    "eval(random.choice(docs_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888d725d-51c7-43a7-b546-56a17d131274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BM25\n",
    "with open(\"passages.txt\",\"r\") as f:\n",
    "\tlst_chunks = f.read().split(\"<endofpassage>\")[:-1]\n",
    "mapping_chunks = {j:i for i,j in enumerate(list(set(lst_chunks)))}\n",
    "lst_chunks = list(set(lst_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a95db9c-bdb7-4fb8-a74d-f5f6214c3109",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visual length of context\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "# %%matplotlib.inline()\n",
    "\n",
    "length = [len(x.split(\" \")) for x in lst_chunks]\n",
    "plt.hist(length, bins=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9351926-e8ec-4da7-bb19-581ee19256eb",
   "metadata": {},
   "source": [
    "#### 3.2 Excuting Baseline - IRCOT\n",
    "ref: https://github.com/stonybrooknlp/ircot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c635b8-65cd-408e-83d0-33b5e7c30b85",
   "metadata": {},
   "source": [
    "##### 3.2.1 Retrieve Modulus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1593312f-a726-4be3-b019-dd627794995b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Retrieval \n",
    "### Using BM25\n",
    "from rank_bm25 import BM25Okapi\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "tokenized_corpus = [doc.split(\" \") for doc in lst_chunks]\n",
    "bm25 = BM25Okapi(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fbc603-05e5-4127-8912-8407c64c4b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval_bm25(question, k):\n",
    "\ttokenized_query = question.split(\" \")\n",
    "\tlst_retrieval = bm25.get_top_n(tokenized_query, lst_chunks, n=k)\n",
    "\treturn lst_retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe49cdb-cc63-4869-b162-e158ba31f942",
   "metadata": {},
   "source": [
    "#### BGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dcef6c-ca8d-46c5-9210-448dec8f168a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlagEmbedding import BGEM3FlagModel\n",
    "\n",
    "model = BGEM3FlagModel('/workspace/home/NLP_CORE/HUB_Embedding/bge-large-en-v1.5/',\n",
    "\t\t\t\t\t   use_fp16=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93af755-bc88-477d-a241-a5494f42eccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "passages = lst_chunks\n",
    "sentences_1 = passages\n",
    "embeddings_1 = embeddings.embed_documents(lst_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2060e327-61e5-46c7-82c5-30226f624145",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_embd = []\n",
    "for i in tqdm(range(len(embeddings_1))):\n",
    "\tp_embd.append(embeddings_1[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef57cff-215e-441f-94f9-f24968154ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_embd = np.array(p_embd)\n",
    "p_embd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb62b9c-4441-4f30-86c7-479255576019",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_p = faiss.IndexFlatIP(p_embd.shape[1])  # IP = Inner Product for cosine similarity\n",
    "index_p.add(p_embd)  # Add encoded passages to the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96302110-659e-4174-ad0b-7fffc552ef27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval_bge(query, k, alpha=0.7):\n",
    "\tquery = query.split(\" with some extra data: \")\n",
    "\tif len(query) > 1:\n",
    "\t\tquestion = query[0]\n",
    "\t\tcontext = query[1]\n",
    "\t\tq_embd = np.array(embeddings.embed_query(max_len(question)))\n",
    "\t\tif len(context) > 0:\n",
    "\t\t\tc_embd = np.array(embeddings.embed_query(max_len(context)))\n",
    "\t\t\tv_fuse = alpha*q_embd + (1-alpha)*c_embd\n",
    "\t\t\tv_fuse = v_fuse.reshape(1, -1)\n",
    "\t\telse:\n",
    "\t\t\tv_fuse = q_embd.reshape(1,-1)\n",
    "\telse:\n",
    "\t\tquestion = query[0]\n",
    "\t\tq_embd = np.array(embeddings.embed_query(max_len(question)))\n",
    "\t\tv_fuse = q_embd.reshape(1,-1)\n",
    "\tdistances, indices = index_p.search(v_fuse, k)\n",
    "\tindices = indices[0]\n",
    "\tretrieved_docs = []\n",
    "\tfor idx in indices:\n",
    "\t\tretrieved_docs.append(lst_chunks[idx])\n",
    "\treturn retrieved_docs\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e7b420-b61b-45c2-b3f7-101d852a6ee3",
   "metadata": {},
   "source": [
    "##### 3.2.12 Interleaving Retrieval with Chain-of-Thought Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf623c44-1d2f-47ca-8f3c-78178a73014f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser,JsonOutputParser\n",
    "\n",
    "\t\n",
    "def format_docs(docs):\n",
    "\t# return \"\\n\\n\".join(f\"[{i+1}] {doc.page_content}\" for i, doc in enumerate(docs))\n",
    "\treturn \"\\n\\n\".join(f\"{doc}\" for doc in docs)\n",
    "\n",
    "class GradeRespose(BaseModel):\n",
    "\t\"\"\"Binary score to determine if the passages provide sufficient information to answer the question directly.\"\"\"\n",
    "\tbinary_score: bool = Field(\n",
    "\t\tdescription=\"The relevant passages provide sufficient information to answer the question directly, 'yes' or 'no'\"\n",
    "\t)\n",
    "\n",
    "class gen_query(BaseModel):\n",
    "\t\"\"\"Generate  chain-of-thought query for futher research and exploration.\"\"\"\n",
    "\tnew_query: str = Field(\n",
    "\t\tdescription=\"Generate new chain-of-thought query for futher research and exploration\"\n",
    "\t)\n",
    "\n",
    "def check_response(question, context):\n",
    "\tsystem_promt=(\"You are an advanced AI assistant skilled in analyzing textual data.\"\n",
    "\t\t\"\\nBelow is a question and relevant passages that may contain information to answer it.\"\n",
    "\t\t\"\\nYour task is to determine if the provided passages contain enough relevant information to answer the question, even if not directly stated.\"\n",
    "\t\t\"\\nConsider both direct answers and implied or partially inferred information.\"\n",
    "\t\t\"\\nReturn a binary score: 'True' if the context provides sufficient information to answer the question; 'False' if it does not.\"\n",
    "\t\t\"\\nProvide only the binary score in JSON format with a single key 'score'. Do not include explanations.\")\n",
    "\t\n",
    "\tchat_template_check = tokenizer.apply_chat_template(\n",
    "\t\t[\n",
    "\t\t\t{\"role\":\"system\", \"content\":\"{system_promt}\"},\n",
    "\t\t\t{\"role\":\"user\", \"content\":\"\\nQuestion: {question}\\nRelevan Passages: {context}\"}\n",
    "\t\t], tokenize=False, add_generation_prompt=True)\n",
    "\t\n",
    "\tprompt_check_response = PromptTemplate(template=chat_template_check, input_variables=[\"system_promt\", \"question\",\"context\"])\n",
    "\tstructured_check_content= llm.with_structured_output(GradeRespose)\n",
    "\tcheck_response_chain = prompt_check_response | structured_check_content \n",
    "\tresults = check_response_chain.invoke({\"system_promt\": system_promt, \"question\": question ,\"context\": context})\n",
    "\treturn results\n",
    "\n",
    "def gen_question(question, context, previous_though):\n",
    "\tsystem_promt_gen_answer = (\n",
    "\t\t\"You are an advanced AI skilled in generating a concise insightful chain-of-thought query to guide further research and exploration.\"\n",
    "\t\t\" Below is an input question and relevant context information and previous failed queries.\"\n",
    "\t\t\"\\nYour task is to :\"\n",
    "\t\t\"\\n1. Analyze the input question to understand its intent and identify gaps in the provided context that prevent a complete answer.\"\n",
    "\t\t\"\\n2. Generate a new chain-of-thought query that is based on the input question, incorporating logical steps or deeper aspects of the topic.\"\n",
    "\t\t\" This new query should be designed to guide further search or inquiry, aiming to bridge the identified gaps and refine the search for an answer.\"\n",
    "\t\t\"\\n3. Avoid repeating or rephrasing any of the previous failed queries. Instead, aim to expand the scope or explore different facets of the topic that have not been addressed yet.\"\n",
    "\t\t\"All JSON MUST in correct format\"\n",
    "\t\t\"**DO NOT get information from 'Relevant context information' to create new input variables.**\"\n",
    "\t)\n",
    "\n",
    "\tchat_gen_answer = tokenizer.apply_chat_template(\n",
    "\t\t[\n",
    "\t\t\t{\"role\": \"system\", \"content\": system_promt_gen_answer},\n",
    "\t\t\t{\"role\": \"user\", \"content\": f\"\\nQuestion: {question}\\nRelevant context information: {context}\\nPrevious failed queries: {previous_though}\"}\n",
    "\t\t],\n",
    "\t\ttokenize=False, \n",
    "\t\tadd_generation_prompt=True\n",
    "\t)\n",
    "\t# print(chat_gen_answer)\n",
    "\t# print(\"\\n\\n\\n\")\n",
    "\t\n",
    "\tprompt_gen_answer = PromptTemplate(template=chat_gen_answer, input_variables=[\"system_promt_gen_answer\", \"question\", \"context\", \"previous_though\"])\n",
    "\t# print(prompt_gen_answer)\n",
    "\tstructured_check_content = llm.with_structured_output(gen_query)\n",
    "\tchain_gen_answer = prompt_gen_answer | structured_check_content\n",
    "\tanswer = chain_gen_answer.invoke({\"system_promt_gen_answer\": system_promt_gen_answer, \"question\": question, \"context\": context, \"previous_though\": previous_though})\n",
    "\n",
    "\treturn answer\n",
    "\n",
    "\n",
    "def final_answer(question, context):\n",
    "\tsystem_promt_gen_answer=(\"You are an expert AI designed to analyze information from retrieval-augumented generation system.\"\n",
    "\t\"\\nYour task is to answer questions based on the input context. Below is a question along with the input context.\"\n",
    "\t\"\\nMake sure your repsonse is consice clear, and directly answer the question in maximum 5 sentences WITHOUT any explaination.\"\n",
    "\t\"\\nDO NOT use any external knowledge. \"\n",
    "\t\"\\nIf the answer is not directly found, try to infer the best possible answer from the context.\")\n",
    "\t\n",
    "\tchat_gen_answer= tokenizer.apply_chat_template(\n",
    "\t\t[\n",
    "\t\t\t{\"role\":\"system\", \"content\":\"{system_promt_gen_answer}\"},\n",
    "\t\t\t{\"role\":\"user\", \"content\":\"\\nQuestion: {question}\\nInput context: {context}\"}\n",
    "\t\t], tokenize=False, add_generation_prompt=True)\n",
    "\tprompt_gen_answer = PromptTemplate(template=chat_gen_answer, input_variables=[\"system_promt_gen_answer\", \"question\",\"context\"])\n",
    "\t# print(prompt_gen_answer.invoke({\"system_promt_gen_answer\": system_promt_gen_answer,\"question\":question, \"context\": context}))\n",
    "\tchain_gen_answer = prompt_gen_answer | llm | StrOutputParser()\n",
    "\tanswer = chain_gen_answer.invoke({\"system_promt_gen_answer\": system_promt_gen_answer,\"question\":question, \"context\": context}).strip()\n",
    "\treturn answer\n",
    "\n",
    "def max_length_context(context,threshold=512):\n",
    "\t### context : list\n",
    "\tres = []\n",
    "\tfor i in context:\n",
    "\t\tif len(i.split(\" \")) > threshold:\n",
    "\t\t\ttmp = \" \".join(x for x in i.split(\" \")[:threshold])\n",
    "\t\t\tres.append(tmp)\n",
    "\t\telse:\n",
    "\t\t\tres.append(i)\n",
    "\treturn res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0a38e3-4ca8-4b96-a9b4-8cd45534f2da",
   "metadata": {},
   "source": [
    "# IRCoT Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf6daa9-8480-4719-a40d-b8eb868cef40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "import pickle\n",
    "import traceback\n",
    "from multiprocessing import Pool, Manager\n",
    "\n",
    "def process_question(tasks):\n",
    "\t\"\"\"Process a single question.\"\"\"\n",
    "\tquestion, label, k, n_loop, qid = tasks  # Unpack the arguments\n",
    "\ttry:\n",
    "\t\ti = 0\n",
    "\t\tthought_q = \"\"\n",
    "\t\tpt = []\n",
    "\t\tgen_answer = None  # Ensure it's always defined\n",
    "\n",
    "\t\tcontext = max_length_context(retrieval_bge(question, k))\n",
    "\t\twhile i < n_loop:\n",
    "\t\t\tcheck = check_response(question, format_docs(context)).binary_score\n",
    "\t\t\tif check or (not check and i == n_loop - 1):\n",
    "\t\t\t\tgen_answer = final_answer(question, format_docs(context))\n",
    "\t\t\t\tbreak\n",
    "\t\t\telse:\n",
    "\t\t\t\tnew_CoT_query = gen_question(question, format_docs(context), \"\\n\".join(pt)).new_query\n",
    "\t\t\t\tpt.append(new_CoT_query)\n",
    "\t\t\t\tthought_q += f\"\\n{i}-{new_CoT_query}\"\n",
    "\t\t\t\tnew_context = max_length_context(retrieval_bge(new_CoT_query, k))\n",
    "\t\t\t\tcontext = list(set(context + new_context))  # Deduplicate\n",
    "\t\t\ti += 1\n",
    "\n",
    "\t\tres = {\n",
    "\t\t\t\"Question\": question,\n",
    "\t\t\t\"id\": qid,\n",
    "\t\t\t\"Answer\": gen_answer,\n",
    "\t\t\t\"Label\": label,\n",
    "\t\t\t\"Context\": context,\n",
    "\t\t\t\"CoT\": thought_q,\n",
    "\t\t\t\"n_CoT\": int(i),\n",
    "\t\t}\n",
    "\texcept Exception as e:\n",
    "\t\tprint(f\"Error occurred during processing question '{question}': {e}\")\n",
    "\t\ttraceback.print_exc()\n",
    "\t\tres = None\n",
    "\n",
    "\tfn = uuid.uuid4()\n",
    "\twith open(f\"llama3.2-3brb_bge/{fn}.pkl\", \"wb\") as f:\n",
    "\t\tpickle.dump(res, f)  # Corrected from dumping `fn` to dumping `res`\n",
    "\treturn res\n",
    "\n",
    "\t\n",
    "\n",
    "\n",
    "def main():\n",
    "\t# Parameters\n",
    "\tk = 8 \n",
    "\tn_loop = 5 \n",
    "\tnum_procs = 8\n",
    "\tquestions = df_test['question'].tolist()[:600]\n",
    "\tlabels = df_test[\"response\"].tolist()[:600]\n",
    "\tids = df_test[\"id\"].tolist()[:600]\n",
    "\ttasks = [(questions[i], labels[i], k, n_loop, ids[i]) for i in range(len(questions))]\n",
    "\n",
    "\t# Use a Manager list to store results\n",
    "\twith Manager() as manager:\n",
    "\t\twith Pool(20) as pool:\n",
    "\t\t\tresults = list(tqdm(pool.imap(process_question, tasks), total=len(tasks)))\n",
    "\n",
    "\t\tresults = [res for res in results if res is not None]\n",
    "\t\tfinal_test = pd.DataFrame(results)\n",
    "\n",
    "\t\t# Save to an Excel file\n",
    "\t\tfinal_test.to_excel(\"IRCoT_CQR_Inference_1605_bge.xlsx\", index=False)\n",
    "\t\tprint(\"Processing complete. Results saved to 'IRCoT_baseline_inference_llama3-70b.xlsx'.\")\n",
    "\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "\tmain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48bb3c5-c57a-452d-a9e3-9341ad87c7ae",
   "metadata": {},
   "source": [
    "# IRCoT + KG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1d90c2-13b6-4d7c-b053-9d0b515d9d53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "import pickle\n",
    "import traceback\n",
    "from multiprocessing import Pool, Manager\n",
    "\n",
    "def process_question(tasks):\n",
    "\t\"\"\"Process a single question.\"\"\"\n",
    "\tquestion, label, k, n_loop, qid = tasks  # Unpack the arguments\n",
    "\ttry:\n",
    "\t\ti = 0\n",
    "\t\tthought_q = \"\"\n",
    "\t\tpt = []\n",
    "\t\tgen_answer = None  # Ensure it's always defined\n",
    "\n",
    "\t\tcontext = max_length_context(retrieval_bge(add_triplet_context_to_question(question), k))\n",
    "\t\twhile i < n_loop:\n",
    "\t\t\tcheck = check_response(question, format_docs(context)).binary_score\n",
    "\t\t\tif check or (not check and i == n_loop - 1):\n",
    "\t\t\t\tgen_answer = final_answer(question, format_docs(context))\n",
    "\t\t\t\tbreak\n",
    "\t\t\telse:\n",
    "\t\t\t\tnew_CoT_query = gen_question(question, format_docs(context), \"\\n\".join(pt)).new_query\n",
    "\t\t\t\tpt.append(new_CoT_query)\n",
    "\t\t\t\tthought_q += f\"\\n{i}-{new_CoT_query}\"\n",
    "\t\t\t\tnew_context = max_length_context(retrieval_bge(add_triplet_context_to_question(new_CoT_query), k))\n",
    "\t\t\t\tcontext = list(set(context + new_context))  # Deduplicate\n",
    "\t\t\ti += 1\n",
    "\n",
    "\t\tres = {\n",
    "\t\t\t\"Question\": question,\n",
    "\t\t\t\"id\": qid,\n",
    "\t\t\t\"Answer\": gen_answer,\n",
    "\t\t\t\"Label\": label,\n",
    "\t\t\t\"Context\": context,\n",
    "\t\t\t\"CoT\": thought_q,\n",
    "\t\t\t\"n_CoT\": int(i),\n",
    "\t\t}\n",
    "\texcept Exception as e:\n",
    "\t\tprint(f\"Error occurred during processing question '{question}': {e}\")\n",
    "\t\t# traceback.print_exc()\n",
    "\t\tres = None\n",
    "\n",
    "\tfn = uuid.uuid4()\n",
    "\twith open(f\"llama3.2-3brb_bge_kcqr/{fn}.pkl\", \"wb\") as f:\n",
    "\t\tpickle.dump(res, f)  # Corrected from dumping `fn` to dumping `res`\n",
    "\treturn res\n",
    "\n",
    "\t\n",
    "\n",
    "\n",
    "def main():\n",
    "\t# Parameters\n",
    "\tk = 8  # Set top-k retrieval\n",
    "\tn_loop = 5  # Number of loops\n",
    "\tnum_procs = 8  # Number of processes to use\n",
    "\t# Convert test data into a list of arguments for the worker function\n",
    "\tquestions = df_test['question'].tolist()[:600]\n",
    "\tlabels = df_test[\"response\"].tolist()[:600]\n",
    "\tids = df_test[\"id\"].tolist()[:600]\n",
    "\ttasks = [(questions[i], labels[i], k, n_loop, ids[i]) for i in range(len(questions))]\n",
    "\n",
    "\t# Use a Manager list to store results\n",
    "\twith Manager() as manager:\n",
    "\t\t# Create a multiprocessing pool\n",
    "\t\twith Pool(20) as pool:\n",
    "\t\t\t# Process questions in parallel with progress tracking\n",
    "\t\t\tresults = list(tqdm(pool.imap(process_question, tasks), total=len(tasks)))\n",
    "\n",
    "\t\t# Filter out None results (in case of errors)\n",
    "\t\tresults = [res for res in results if res is not None]\n",
    "\n",
    "\t\t# Convert the results into a DataFrame\n",
    "\t\tfinal_test = pd.DataFrame(results)\n",
    "\n",
    "\t\t# Save to an Excel file\n",
    "\t\tfinal_test.to_excel(\"IRCoT_CQR_Inference_1605_bge_kcqr.xlsx\", index=False)\n",
    "\t\tprint(\"Processing complete. Results saved to 'IRCoT_baseline_inference_llama3-70b.xlsx'.\")\n",
    "\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "\tmain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f2d241-a7e9-48a3-a989-e3be1bb51c97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
